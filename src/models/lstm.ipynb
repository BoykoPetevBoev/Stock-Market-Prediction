{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Boyko Boev\\Stock-Market-Prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Boyko Boev\\Stock-Market-Prediction\\venv\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential \n",
    "\n",
    "Sequential моделът в TensorFlow е основният начин за създаване на невронни мрежи, които са линейни стекове от слоеве. Той представлява проста архитектура на модела, където следващият слой винаги е свързан с предишния.\n",
    "\n",
    "За да създадем Sequential модел, използваме класа Sequential от tensorflow.keras.models. Ето как можем да го използваме:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "model = tf.keras.models.Sequential()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM (Long Short-Term Memory) е вид рекурентна невронна мрежа (RNN), която е специално проектирана за работа с последователни данни и запазване на дългосрочни зависимости между данните. Тя е полезна за решаване на задачи като времеви редове, текстова обработка и други, където важните информационни зависимости са разпръснати във времето.\n",
    "\n",
    "Ето пример как се създава LSTM слой в модел на Keras:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "model.add(LSTM(units=64, input_shape=(timesteps, features)))\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **units:** Брой неврони във вътрешния слой на LSTM. Този параметър контролира размера на скритото пространство на LSTM модела и служи за регулиране на сложността на модела.\n",
    "\n",
    "- **input_shape:** Форматът на входните данни за LSTM слоя. Параметърът input_shape трябва да бъде предоставен само за първия LSTM слой в модела. Той определя броя времеви стъпки и броя характеристики на входните данни.\n",
    "\n",
    "- **return_sequences**: Определя дали LSTM слоят трябва да връща изходи за всички времеви стъпки или само за последната. По подразбиране, този параметър е зададен на False, което означава, че LSTM слоят ще върне изход само за последната времева стъпка.\n",
    "\n",
    "- **activation:** Активационната функция, която се прилага върху изходите на невроните в LSTM слоя. По подразбиране, този параметър е зададен на 'tanh' (хиперболичен тангенс), което е стандартната активационна функция за LSTM."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слоят Dense (пълносвързан слой) е основен вид слой в невронните мрежи, където всеки неврон в предходния слой е свързан с всеки неврон в текущия слой. Той е често използван за изграждане на модели за класификация, регресия и други задачи на машинното обучение.\n",
    "\n",
    "Той е много гъвкав и може да се използва в различни архитектури на невронни мрежи. Когато създавате модел, можете да добавите няколко слоя Dense един след друг, или да ги комбинирате с други типове слоеве като Conv2D или LSTM, за да създадете по-сложни модели.\n",
    "\n",
    "```\n",
    "tf.keras.layers.Dense(\n",
    "    units,\n",
    "    activation=None,\n",
    "    use_bias=True,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    bias_initializer='zeros',\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    **kwargs\n",
    ")\n",
    "```\n",
    "- units\tПоложително цяло число, размерност на изходното пространство.\n",
    "- activation\tФункция за активация. Ако не посочите нищо, не се прилага активация (т.е. линейна активация: a(x) = x).\n",
    "- use_bias\tБулева стойност, дали слоят използва вектор за отместване (bias).\n",
    "- kernel_initializer\tИнициализатор за матрицата на теглата на ядрото.\n",
    "- bias_initializer\tИнициализатор за вектора за отместване (bias).\n",
    "- kernel_regularizer\tФункция за регуляризация, приложена към матрицата на теглата на ядрото.\n",
    "- bias_regularizer\tФункция за регуляризация, приложена към вектора за отместване (bias).\n",
    "- activity_regularizer\tФункция за регуляризация, приложена към изхода на слоя (неговата \"активация\").\n",
    "- kernel_constraint\tФункция за ограничаване, приложена към матрицата на теглата на ядрото.\n",
    "- bias_constraint\tФункция за ограничаване, приложена към вектора за отместване (bias).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout е техника за регуляризация, която се използва за намаляване на преобучването в невронни мрежи. Проблемът с преобучването възниква, когато моделът се научи да \"запомни\" тренировъчните данни толкова добре, че не може да генерализира на нови данни."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Техниката на Dropout работи по следния начин: по време на обучение, случайно изключва (с нулева вероятност) неврони от определен слой за всяка итерация на обучение. Това означава, че някои неврони се игнорират, а други се активират. Тази случайност помага на модела да научи по-робустни и общи характеристики, като се преодолява преобучването."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **rate:** Това е дроб, който указва вероятността за изключване на неврон по време на обучение. Например, ако rate е 0.5, тогава всеки неврон има 50% вероятност да бъде изключен при всяка итерация на обучение.\n",
    "\n",
    "- **noise_shape:** Параметърът noise_shape позволява допълнително форматиране на rate, за да се изключат неврони от различни части на входния тензор.\n",
    "\n",
    "- **seed:** Това е цяло число, което се използва за инициализиране на генератора на случайни числа. Това позволява възпроизводимостта на резултатите, като същевременно използва Dropout."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout е мощна техника за предотвратяване на преобучването и за постигане на по-добра обобщаваща способност на модела. Той може да бъде използван с различни видове невронни мрежи, включително пълносвързани мрежи, конволюционни невронни мрежи (CNN) и рекурентни невронни мрежи (RNN)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Architecture: Make sure your model architecture is suitable for time series forecasting. LSTM layers are commonly used for such tasks due to their ability to capture temporal dependencies.\n",
    "\n",
    "Data Preprocessing: Ensure that your data preprocessing steps, such as normalization and sequence preparation, are appropriate for the problem at hand.\n",
    "\n",
    "Loss Function: Choose an appropriate loss function for your regression problem. Mean Squared Error (MSE) is commonly used for regression tasks.\n",
    "\n",
    "Training Process: Train your model using an appropriate optimizer and monitor its performance on a validation set to avoid overfitting.\n",
    "\n",
    "Hyperparameter Tuning: Experiment with different hyperparameters such as learning rate, batch size, number of LSTM units, and number of layers to improve your model's performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73f5850e139343e7d56cdbfd592890725b73c7b65ece657753fc54e096f228ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
