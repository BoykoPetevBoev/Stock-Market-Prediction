{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Boyko Boev\\Stock-Market-Prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Boyko Boev\\Stock-Market-Prediction\\venv\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Дефиниране на модела"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential \n",
    "\n",
    "[tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential)\n",
    "\n",
    "Sequential моделът в TensorFlow е основният начин за създаване на невронни мрежи, които са линейни стекове от слоеве. Той представлява проста архитектура на модела, където следващият слой винаги е свързан с предишния.\n",
    "\n",
    "За да създадем Sequential модел, използваме класа Sequential от tensorflow.keras.models. Ето как можем да го използваме:\n",
    "\n",
    "*Пример:*\n",
    "\n",
    "```\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "```\n",
    "\n",
    "### LSTM\n",
    "\n",
    "[tf.keras.layers.LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n",
    "\n",
    "LSTM (Long Short-Term Memory) е вид рекурентна невронна мрежа (RNN), която е специално проектирана за работа с последователни данни и запазване на дългосрочни зависимости между данните. Тя е полезна за решаване на задачи като времеви редове, текстова обработка и други, където важните информационни зависимости са разпръснати във времето.\n",
    "\n",
    "*Пример:*\n",
    "\n",
    "```\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "LSTM(\n",
    "    units=64, \n",
    "    input_shape=(timesteps, features),\n",
    "    return_sequences=False\n",
    ")\n",
    "```\n",
    "\n",
    "*Параметри:*\n",
    "\n",
    "- **units:** Броят на невроните в LSTM слоя. По-високата стойност може да доведе до по-добра точност, но и до по-бавно обучение.\n",
    "\n",
    "- **input_shape:** Форматът на входните данни за LSTM слоя. Параметърът input_shape трябва да бъде предоставен само за първия LSTM слой в модела. Той определя броя времеви стъпки и броя характеристики на входните данни.\n",
    "\n",
    "- **return_sequences**: Този параметър контролира дали LSTM слойът ще върне изхода за всяка стъпка във входната последователност или само изхода за последната стъпка.\n",
    "\n",
    "- **activation:** Активационната функция, която се прилага върху изходите на невроните в LSTM слоя. По подразбиране, този параметър е зададен на 'tanh' (хиперболичен тангенс), което е стандартната активационна функция за LSTM.\n",
    "\n",
    "    - **Tanh (хиперболичен тангенс):** Това е стандартната активационна функция за LSTM слоеве. Тя е функция с формата на S, която варира между -1 и 1. Тя е добра за избягване на проблема с изчезващия градиент, който може да възникне в дълги LSTM мрежи.\n",
    "\n",
    "    - **ReLU (Rectified Linear Unit):** Тази функция е с нулева стойност за отрицателни входове и линейна връзка за положителни входове. Тя е популярна поради своята простота и ефективност на обучението. Въпреки това, тя може да страда от проблема с мъртвите неврони, където някои неврони никога не активират след обучение.\n",
    "\n",
    "    - **Sigmoid:** Тази функция е подобна на tanh, dar варира между 0 и 1. Тя беше по-често срещана в миналото, но сега обикновено се предпочита tanh или ReLU поради по-бързото им обучение.\n",
    "\n",
    "\n",
    "### Dense\n",
    "\n",
    "[tf.keras.layers.Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)\n",
    "\n",
    "Слоят Dense (пълносвързан слой) е основен вид слой в невронните мрежи, където всеки неврон в предходния слой е свързан с всеки неврон в текущия слой. Той е често използван за изграждане на модели за класификация, регресия и други задачи на машинното обучение.\n",
    "\n",
    "Той е много гъвкав и може да се използва в различни архитектури на невронни мрежи. Когато създавате модел, можете да добавите няколко слоя Dense един след друг, или да ги комбинирате с други типове слоеве като Conv2D или LSTM, за да създадете по-сложни модели.\n",
    "\n",
    "*Пример:*\n",
    "\n",
    "```\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "Dense(\n",
    "    units=64,\n",
    "    activation='relu',\n",
    ")\n",
    "```\n",
    "\n",
    "*Параметри:*\n",
    "\n",
    "- **units**\tПоложително цяло число, размерност на изходното пространство.\n",
    "\n",
    "- **activation:** Активационната функция, която се прилага върху изходите на невроните в LSTM слоя. По подразбиране, този параметър е зададен на 'tanh' (хиперболичен тангенс), което е стандартната активационна функция за LSTM.\n",
    "\n",
    "    - **Tanh (хиперболичен тангенс):** Това е стандартната активационна функция за LSTM слоеве. Тя е функция с формата на S, която варира между -1 и 1. Тя е добра за избягване на проблема с изчезващия градиент, който може да възникне в дълги LSTM мрежи.\n",
    "\n",
    "    - **ReLU (Rectified Linear Unit):** Тази функция е с нулева стойност за отрицателни входове и линейна връзка за положителни входове. Тя е популярна поради своята простота и ефективност на обучението. Въпреки това, тя може да страда от проблема с мъртвите неврони, където някои неврони никога не активират след обучение.\n",
    "\n",
    "    - **Sigmoid:** Тази функция е подобна на tanh, dar варира между 0 и 1. Тя беше по-често срещана в миналото, но сега обикновено се предпочита tanh или ReLU поради по-бързото им обучение.\n",
    "\n",
    "### Dropout\n",
    "\n",
    "[tf.keras.layers.Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout)\n",
    "\n",
    "Dropout е техника за регуляризация, която се използва за намаляване на преобучването в невронни мрежи. Проблемът с преобучването възниква, когато моделът се научи да \"запомни\" тренировъчните данни толкова добре, че не може да генерализира на нови данни.\n",
    "\n",
    "Техниката на Dropout работи по следния начин: по време на обучение, случайно изключва (с нулева вероятност) неврони от определен слой за всяка итерация на обучение. Това означава, че някои неврони се игнорират, а други се активират. Тази случайност помага на модела да научи по-робустни и общи характеристики, като се преодолява преобучването.\n",
    "\n",
    "Dropout е мощна техника за предотвратяване на преобучването и за постигане на по-добра обобщаваща способност на модела. Той може да бъде използван с различни видове невронни мрежи, включително пълносвързани мрежи, конволюционни невронни мрежи (CNN) и рекурентни невронни мрежи (RNN).\n",
    "\n",
    "*Параметри:*\n",
    "\n",
    "- **rate:** Това е дроб, който указва вероятността за изключване на неврон по време на обучение. Например, ако rate е 0.5, тогава всеки неврон има 50% вероятност да бъде изключен при всяка итерация на обучение.\n",
    "\n",
    "- **noise_shape:** Параметърът noise_shape позволява допълнително форматиране на rate, за да се изключат неврони от различни части на входния тензор.\n",
    "\n",
    "- **seed:** Това е цяло число, което се използва за инициализиране на генератора на случайни числа. Това позволява възпроизводимостта на резултатите, като същевременно използва Dropout.\n",
    "\n",
    "### Reshape\n",
    "\n",
    "[tf.keras.layers.Reshape](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Reshape)\n",
    "\n",
    "Слойът Reshape в TensorFlow се използва за промяна на формата на тензор, това е полезно за преобразуване на данните в подходящ формат за последващи слоеве в модела.\n",
    "\n",
    "Слойът Reshape не променя общия брой елементи в тензора. Той само пренарежда елементите по различни оси.\n",
    "\n",
    "Използването на Reshape е често срещано при работа с данни, които трябва да бъдат преобразувани в специфична форма за определени видове слоеве, като например LSTM или CNN.\n",
    "\n",
    "Експериментирането с различни стойности за target_shape може да повлияе на производителността и точността на модела.\n",
    "\n",
    "*Пример:*\n",
    "```\n",
    "from tensorflow.keras.layers import Reshape\n",
    "\n",
    "Reshape(\n",
    "    target_shape=(x, y, z)\n",
    "    input_shape=(None, x, y)\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "*Параметри:*\n",
    "\n",
    "- **target_shape:** Това е tuple със целочислени стойности, указващи целевата форма на тензора след преобразуване.\n",
    "\n",
    "- **input_shape:** Това е tuple със целочислени стойности, указващи формата на входния тензор. Този параметър е необходим само ако Reshape е първият слой в модела и променя формата на данните или някой предишен слой в модела вече е променил формата на данните и не е посочена изрично с помощта на input_shape параметър\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Компилиране на модела\n",
    "\n",
    "\n",
    "### Оптимизатор\n",
    "\n",
    "[tf.keras.optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n",
    "\n",
    "Алгоритъм, който се използва за актуализиране на теглата на модела по време на обучението.\n",
    "\n",
    "#### Adam\n",
    "\n",
    "(Adaptive Moment Estimation) е метод за стохастичен градиентен descent, който се основава на адаптивна оценка на моменти от първи и втори ред. Adam автоматично адаптира скоростта на обучение за всеки параметър на модела, което може да доведе до по-бърза конвергенция и по-добри резултати. Съхранява само ограничено количество информация за предишни градиенти, което го прави подходящ за големи модели и набори от данни. Инвариантен спрямо мащабирането на отделните елементи на градиента, което може да подобри стабилността на обучението.\n",
    "\n",
    "*Пример:*\n",
    "```\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(\n",
    "    optimizer = Adam(\n",
    "        learning_rate=0.001\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "*Параметри:*\n",
    "\n",
    "- **learning_rate** Това е хиперпараметър, който контролира колко бързо се актуализират теглата на модела по време на обучението. По-високи стойности водят до по-бързо обучение, но могат да доведат до нестабилност и локални минимуми. По-ниски стойности водят до по-бавно обучение и могат да доведат до по-добра точност и по-добра конвергенция.\n",
    "\n",
    "### Функция на загуба\n",
    "\n",
    "#### Средна квадратична грешка (MSE)\n",
    "\n",
    "[tf.keras.losses.MeanSquaredError](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError)\n",
    "\n",
    "MSE измерва средната квадратична разлика между прогнозите на модела и истинските стойности. Лесна за разбиране и изчисляване. Често се използва за задачи за регресия. Не е подходяща за задачи за класификация.\n",
    "\n",
    "*Пример:*\n",
    "```\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "model.compile(\n",
    "    loss = MeanSquaredError()\n",
    ")\n",
    "```\n",
    "\n",
    "*Формула:*\n",
    "```\n",
    "MSE = (1/n) * Σ(y_i - p_i)^2\n",
    "```\n",
    "\n",
    "*Параметри:*\n",
    "- **n** е броят на наблюденията\n",
    "- **y_i** е истинската стойност за i-тото наблюдение\n",
    "- **p_i** е прогнозата на модела за i-тото наблюдение\n",
    "\n",
    "### Метрики\n",
    "\n",
    "Параметъра, който приема списък с метрики, използвани за оценка на производителността на модела.\n",
    "\n",
    "#### Средна абсолютна грешка\n",
    "\n",
    "[tf.keras.metrics.MeanAbsoluteError](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanAbsoluteError)\n",
    "\n",
    "MAE (Mean Absolute Error) е алтернативна функция на загуба за задачи за регресия. Тя измерва средната абсолютна разлика между прогнозите на модела и истинските стойности. Често се използва заедно с други метрики, като например MSE, за по-цялостна оценка на производителността на модела. Тя може да бъде полезна, когато има големи изброи в данните, тъй като е по-малко чувствителна към тях.\n",
    "\n",
    "*Пример:*\n",
    "```\n",
    "from tensorflow.keras.metrics import MeanAbsoluteError\n",
    "\n",
    "model.compile(\n",
    "    metrics=[MeanAbsoluteError()]\n",
    ")\n",
    "```\n",
    "\n",
    "*Формула:*\n",
    "```\n",
    "MAE = (1/n) * Σ|y_i - p_i|\n",
    "```\n",
    "\n",
    "*Параметри:*\n",
    "- **n** е броят на наблюденията\n",
    "- **y_i** е истинската стойност за i-тото наблюдение\n",
    "- **p_i** е прогнозата на модела за i-тото наблюдение\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73f5850e139343e7d56cdbfd592890725b73c7b65ece657753fc54e096f228ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
